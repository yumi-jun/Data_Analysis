{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OtBGOufV6y26XmRJj7agG7Oe6u0OBeGQ",
      "authorship_tag": "ABX9TyM7OGyFqHWk0Rtm7aqhMMMQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yumi-jun/Data_Analysis/blob/main/CNN_cat_and_dog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "라이브러리 적용"
      ],
      "metadata": {
        "id": "m48XsqENvQG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MsJV4A8cu58y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ff9fcf-1a2d-4ad6-c634-5bfd497e0b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/deepLearing/dataset'\n"
      ],
      "metadata": {
        "id": "dGfz58as-jCk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part1 : 데이터 전처리\n"
      ],
      "metadata": {
        "id": "vaQ66uqrvb62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터"
      ],
      "metadata": {
        "id": "R31tMDVdvwad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 전처리 -> 가장 먼저 훈련 세트의 모든 이미지를 변형\n",
        "# 과적합을 방지 하기 위해\n",
        "\n",
        "# train_datagen : 훈련 세트의 이미지에 모든 변화를 적용할 도구를 나타내는 변수\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255, # 특징 스케일링, 픽셀의 값을 255로 나누어 작용, 각 픽셀은 0~255 로 적용,\n",
        "        # 모든 픽셀 값이 0과 1 사이로 나오게 됨.\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        " #이미지 증강 도구를 이미지에 적용\n",
        "training_set=train_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/deepLearing/dataset/training_set', # 훈련 세트로 가는 경로\n",
        "        target_size=(64,64), # 이미지의 최종 크\n",
        "        batch_size=32, # 각 배치에 이미지를 몇 개 배치할 것인\n",
        "        class_mode='binary') # 결과값, 이진 모드, 카테고리 모드\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xMfFycdj2woM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2e4be6-8e8b-42a1-e978-e45ce2f23848"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1755 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 데이터 test data"
      ],
      "metadata": {
        "id": "6k8tM8MR56sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/deepLearing/dataset/test_set',\n",
        "        target_size=(64,64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n"
      ],
      "metadata": {
        "id": "CtBU0ZavvfaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac00ec4-a662-4411-a479-7ef9896bb09c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. CNN  만들기\n"
      ],
      "metadata": {
        "id": "tXDKjb078Vr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN 만들기\n"
      ],
      "metadata": {
        "id": "Sz2Hlh608geE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn=tf.keras.models.Sequential()\n"
      ],
      "metadata": {
        "id": "7iqOGJBv8ahT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - Convolution"
      ],
      "metadata": {
        "id": "WeLQKcV08rw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  filters : 처리할 이미지의 수, kernel_size : 특징 감지기의 수 : 3*3 , 정류화 활성화 함수를 사용, 이미지의 입력 형태 : 64 사이즈의 rgb 색상 :3\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))"
      ],
      "metadata": {
        "id": "oEvWfTdi7LLR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 - Pooling"
      ],
      "metadata": {
        "id": "TreKTJFEAQp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 풀 크기 : 2 -> 추천 사이즈임\n",
        "# 간격 : strides\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "metadata": {
        "id": "vTzv02Vl5ojM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a second convolutional layer\n",
        ": 두번째 컨볼루션 레이어 추가"
      ],
      "metadata": {
        "id": "LTo1jxgCBEIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "왜 두번째 컨볼루션 레이어를 추가하는지..?"
      ],
      "metadata": {
        "id": "9gZNc8rnBWMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "metadata": {
        "id": "CA4r0oXABWAw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 : 펼치기"
      ],
      "metadata": {
        "id": "5Y7gFgR6BjAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "Tqe5jFP5BPtA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 : 전결합층 연"
      ],
      "metadata": {
        "id": "UwUvYEQIBzQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 유닛이 128개,\n",
        "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
      ],
      "metadata": {
        "id": "yzYjysERDUg6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 : Output Layer"
      ],
      "metadata": {
        "id": "7ePVeeNADnTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 유닛이 1개\n",
        "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "fVmgNmt2Dn6F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the CNN : CNN 훈련"
      ],
      "metadata": {
        "id": "B48urKquDTy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OuJ8A62jF6NH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ],
      "metadata": {
        "id": "M0N-u1wqF2IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XBH2DGvFjXT",
        "outputId": "0201b3eb-6f01-4be9-cd68-c117821fbb80"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "55/55 [==============================] - 27s 440ms/step - loss: 0.0158 - accuracy: 0.9823 - val_loss: 2.4070e-32 - val_accuracy: 1.0000\n",
            "Epoch 2/25\n",
            "55/55 [==============================] - 26s 477ms/step - loss: 7.7399e-13 - accuracy: 1.0000 - val_loss: 1.2117e-32 - val_accuracy: 1.0000\n",
            "Epoch 3/25\n",
            "55/55 [==============================] - 25s 450ms/step - loss: 1.0664e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 4/25\n",
            "55/55 [==============================] - 22s 401ms/step - loss: 1.1654e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 5/25\n",
            "55/55 [==============================] - 24s 434ms/step - loss: 5.2280e-13 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 6/25\n",
            "55/55 [==============================] - 22s 404ms/step - loss: 2.8459e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 7/25\n",
            "55/55 [==============================] - 24s 430ms/step - loss: 2.9244e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 8/25\n",
            "55/55 [==============================] - 22s 393ms/step - loss: 1.2776e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 9/25\n",
            "55/55 [==============================] - 24s 436ms/step - loss: 2.3684e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 10/25\n",
            "55/55 [==============================] - 22s 397ms/step - loss: 5.8151e-13 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 11/25\n",
            "55/55 [==============================] - 22s 392ms/step - loss: 1.1871e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "55/55 [==============================] - 24s 435ms/step - loss: 1.2923e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 13/25\n",
            "55/55 [==============================] - 22s 393ms/step - loss: 1.6537e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 14/25\n",
            "55/55 [==============================] - 24s 427ms/step - loss: 8.3211e-13 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 15/25\n",
            "55/55 [==============================] - 22s 393ms/step - loss: 3.0529e-13 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "55/55 [==============================] - 23s 422ms/step - loss: 9.5176e-13 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "55/55 [==============================] - 22s 395ms/step - loss: 1.1966e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "55/55 [==============================] - 24s 427ms/step - loss: 1.2098e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "55/55 [==============================] - 22s 401ms/step - loss: 1.0645e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "55/55 [==============================] - 23s 415ms/step - loss: 7.9757e-13 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "55/55 [==============================] - 22s 406ms/step - loss: 3.7776e-13 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "55/55 [==============================] - 23s 422ms/step - loss: 2.1518e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "55/55 [==============================] - 23s 410ms/step - loss: 9.9159e-13 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "55/55 [==============================] - 23s 421ms/step - loss: 8.5815e-13 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "55/55 [==============================] - 22s 400ms/step - loss: 1.6167e-12 - accuracy: 1.0000 - val_loss: 1.2085e-32 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78985f7c8520>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4: Making a single prediction\n"
      ],
      "metadata": {
        "id": "xZQCprMbggPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image=image.load_img('/content/drive/MyDrive/deepLearing/dataset/test_set/single_prediction/cat_or_dog_2.jpg',target_size=(64,64))\n",
        "\n",
        "# 예측 메소드에서는 , 특정한 입력 방식이 요구된다.\n",
        "# 이미지 데이터 값을 넘파 배열 형태로 변환할 것이다.\n",
        "test_image=image.img_to_array(test_image)\n",
        "test_image=np.expand_dims(test_image,axis=0)\n",
        "result=cnn.predict(test_image)\n",
        "training_set.class_indices\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEQnNl6sgl9Y",
        "outputId": "2b210eb5-ad3e-400a-f4e7-1446892e2252"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dogs': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRFmVZMFhEvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}